<!DOCTYPE html>
<head>
<!-- The title tag is also shown in browser's tab-->
<title>Tomas Geffner | Research Scientist @ NVIDIA</title>
</head>


<body>
<h1>
Tomas Geffner
</h1>
<p>
<!-- Replace with a personal image if desired -->
<!-- <img src="seminole.png" width="300" > -->
<img src="me.jpg" width="300" >
(After an intense hike)
</p>

<hr>



<!-- I finished my PhD in 2023. I am now a research scientist at NVIDIA, working on generative modeling within the team lead by <a href="http://latentspace.cc/">Arash Vahdat</a>. -->



<h1>
About
</h1>
<p>
<!-- <del>I am a final year PhD student in the <a href="https://www.cics.umass.edu">College of Information and Computer Sciences</a> at UMass Amherst. I am fortunate to be advised by <a href="https://people.cs.umass.edu/~domke/">Justin Domke</a>.</del> -->
<del>I am a final year PhD student in the <a href="https://www.cics.umass.edu">College of Information and Computer Sciences</a> at UMass Amherst. I am fortunate to be advised by Justin Domke.</del>
<br><br>
I finished my PhD in 2023. I am now a research scientist at NVIDIA, working on generative modeling within the <a href="https://research.nvidia.com/labs/genair/">Fundamental Generative AI Research Group</a>.
<br><br>
Before joining NVIDIA, I am grateful to have interned at VantAI (New York, summer 2023) with Luca Naef and Michael Bronstein; DeepMind (London, summer 2022) with Andriy Mnih and George Papamakarios; MSR (Cambridge UK, summer 2021) with Cheng Zhang, Emre Kiciman and Miltos Allamanis; and Amazon AWS (New York, summer 2018) with â€ªBing Xiang and Ramesh Nallapati.
<br><br>
I work on probabilistic Machine Learning, focusing on generative modeling, sampling, approximate inference, and causality, with the goal of developing methods able to solve increasingly challenging problems. I am interested in fundamental methodological developments as well as applications in different scientific domains.
<!-- <br><br>
<i>Sampling and approximate inference.</i> I worked on improving sampling and approximate inference techniques, focusing on developing methods that are faster, more robust, flexible, scalable, and overall easier to use.
<br><br>
<i>Generative modeling and causality.</i> I worked on developing methods for conditional generation based on compositional score modeling (applied to simulation-based inference), on causal discovery using normalizing flows and GNNs, and on multi-modal generation and representation learning using VAEs. -->
<br><br>
Before my PhD I also did some work on planning.
<br><br>
This is a <a href="https://scholar.google.com/citations?user=KIIe2K8AAAAJ&hl=es">link</a> to my google scholar.
</p>









<!-- <h1>
About
</h1>
<p>
I am a final year PhD student in the <a href="https://www.cics.umass.edu">College of Information and Computer Sciences</a> at UMass Amherst. I am fortunate to be advised by <a href="https://people.cs.umass.edu/~domke/">Justin Domke</a>.
<br><br>
Most of my work during my PhD has been focused around approximate inference methods. Can we make them faster? More flexible? More robust? Easier to use? 
<br><br>
I'm also interested in generative models and causality. I worked with diffusion models for conditional generation during an internship at DeepMind (summer 2022), with flow-based methods and GNNs for causal discovery during an internship at MSR (summer 2021), and with VAEs for multi modal data during an internship at Amazon AWS (summer 2018).
<br><br>
Before my PhD I also did some work on planning.
<br><br>
This is a <a href="https://scholar.google.com/citations?user=KIIe2K8AAAAJ&hl=es">link</a> to my google scholar.
</p> -->

<hr>

<!-- this is an h1 heading, largest-->
<h1>
Publications (& Preprints)
</h1>
<p>
<ul style="list-style-type:square;">
<li><a href="https://arxiv.org/abs/2209.14249">Compositional Score modeling for Simulation-based Inference.</a> By Tomas Geffner, George Papamakarios and Andriy Mnih. <br>ICML 2023.<br>We introduce a novel method to do simulation-based inference (aka likelihood-free inference) using conditional score modeling. Our method learns the score of (diffused versions) of the target posterior distribution, uses annealed Langevin dynamics to get samples, and can naturally aggregate multiple observations at inference time by exploiting different factorizations of the target distribution.</li> <br>
<li><a href="https://arxiv.org/abs/2208.07743">Langevin Diffusion Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2023.<br>We present a new method to combine VI and MCMC based on the discretization of Langevin diffusion processes and their time reversals. We show that our approach generalizes previously existing methods, and outperforms them on a range of inference tasks.</li> <br>
<li><a href="https://arxiv.org/abs/2203.04432">Variational Inference with Locally Enhanced Bounds for Hierarchical Models.</a> By Tomas Geffner and Justin Domke. <br>ICML 2022 (spotlight).<br>We present a new family of variational objectives for hierarchical models, based on the application of importance weighting (and other tightening methods) at the local variables level. Our approach yields tight lower bounds and powerful posterior approximations while being compatible with subsampling, thus addressing one of the main limitations of tightening methods.</li> <br>
<li><a href="https://arxiv.org/abs/2202.02195">Deep End-to-end Causal Inference.</a> By Tomas Geffner*, Javier Antoran*, Adam Foster*, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltadis Allamanis and Cheng Zhang. <br>Workshop on Causality for Real World Impact (NeurIPS 2022).<br>We present a new end-to-end simulation-based method for causal inference. Our approach performs causal discovery by fitting autoregressive flows, and uses the learned model to generate samples from interventional distributions and estimate treatment effects.</li> <br>
<li><a href="https://arxiv.org/abs/2107.04150">MCMC Variational Inference via Uncorrected Hamiltonian Annealing.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2021.<br>We present a new way of combining VI and MCMC using Annealed Importance Sampling with uncorrected HMC transitions. The method is simple to implement, and all of its parameters (even the ones from the HMC transition kernel!) can be tuned with unbiased reparameterization gradients.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>ICML 2021.<br>Want to minimize an alpha divergence using unbiased gradients? It will be hard. In high dimensions your gradient estimator will likely suffer from an exponentially small SNR.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>AABI 2020 (contributed talk).<br>Want to minimize an alpha divergence using biased gradients? It will be hard. In high dimensions you may end up minimizing the typical exclusive KL divergence.</li> <br>
<li><a href="https://arxiv.org/abs/2007.14634">Approximation Based Variance Reduction for Reparameterization Gradients.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2020.<br>We propose a new control variate to reduce the variance of gradient estimators used by VI. This leads to faster convergence. Our method works better than previously proposed methods based on Taylor expansions. We propose an explanation for this.</li> <br>
<li><a href="https://arxiv.org/abs/1911.01894">A Rule for Gradient Estimator Selection, with an Application to Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2020.<br>Some gradient estimates are cheap but have a high variance. Some are expensive but have lower variance. Others are somewhere in between. Which one should you use? We study this using known SGD convergence rates and propose a simple gradient estimator selection algorithm that aims to minimize optimization wall-clock time.</li> <br>
<li><a href="https://arxiv.org/abs/1810.12482">Using Large Ensembles of Control Variates for Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2018.<br>Using a control variate may lead to a reduction in the variance of gradient estimators, improving optimization convergence. For VI, many control variates are available. We propose a method to use them jointly and show that this may work better than any of the control variates individually.</li> <br>
<li><a href="https://arxiv.org/abs/1806.09455">Compact Policies for Fully-Observable Non-Deterministic Planning as SAT.</a> By Tomas Geffner and Hector Geffner. <br>ICAPS 2018.<br>We introduce a new SAT-based algorithm for FOND planning. The proposed method works well on several probabilistic interesting domains.</li> <br>
<li><a href="http://giga15.ru.is/giga15-paper2.pdf">Width-based Planning for General Video-Game Playing.</a> By Tomas Geffner and Hector Geffner. <br>AAIDE 2015.<br>Developed agent for GVG-AI planning competition (real time autonomous game playing) based on a combination of planning and learning algorithms. The method got the 1st place in CEEC leg of competition (2015).</li> <br>
</ul>
</p>

<!-- <hr> -->

<!-- 
<h1>
Other interesting projects	
</h1>
<p>
<ul style="list-style-type:square;">
<li>Bayesian Modeling to Infer User Preferences. We use a hierarchical probabilistic model to automatically infer users' preferences. The proposed model allows parameter sharing between users and the incorporation of domain knowledge.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>AABI 2020 (contributed talk).<br>Want to minimize an alpha divergence using biased gradients? It will be hard. In high dimensions you may end up minimizing the typical exclusive KL divergence.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>ICML 2021.<br>Want to minimize an alpha divergence using unbiased gradients? It will be hard. In high dimensions your gradient estimator will likely suffer from an exponentially small SNR.</li> <br>
</p>
 -->

<hr>

<h1>
Teaching experience
</h1>
I was a TA for several courses:
<ul style="list-style-type:none;">
<li>Probabilistic graphical models. Office hours and prepared course notes.</li>
<li>Machine learning. Prepared homeworks and exams.</li>
<li>Introduction to data structures. Office hours, discussion sessions, prepared and graded homeworks.</li>
<li>Introduction to Natural language processing. Office hours and grading.</li>
<li>Human Computer Interaction. Mostly grading.</li>
</ul>

<hr>

<h1>
Service
</h1>
Organizing Committee:
<ul style="list-style-type:none;">
<li>Publication chair (AISTATS 2023)</li>
</ul>
<br>
Conference reviewing:
<ul style="list-style-type:none;">
<li>NeurIPS (2019, 2020, 2021, 2022)</li>
<li>ICML (2020)</li>
<li>ICLR (2021, 2022)</li>
<li>AISTATS (2021)</li>
</ul>
<br>
Journal reviewing:
<ul style="list-style-type:none;">
<li>JMLR</li>
<li>TMLR</li>
</ul>

<!-- <hr>

<h1>
Honors and Awards
</h1>
<ul style="list-style-type:circle;">
<li>Winner of the 97th annual FSU Math Competition</li>
<li>Received a teaching award for best TA for the Spring 2018 semester.</li>
<li>Recipient of the FSU Fellowship award.</li>
</ul> -->

<hr>

<h1>
Other
</h1>
<p>
I try to play tennis.
</p>
</p>

<hr>

<h1>
Contact
</h1>
<table>
<!-- <tr><td><bold>Email:</bold> t[lastname]@cs.umass.edu</td> </tr> -->
<tr><td><bold>Email:</bold> t[lastname]@nvidia.com</td> </tr>
<tr><td><bold>Email:</bold> [firstname][lastname]@gmail.com</td> </tr>
</tr>
</table>

<hr>

<h1>
Disclaimer
</h1>
<p>
The template for this website was obtained from <a href="https://www.math.fsu.edu/Computer/personal-webpage-templates/">here.</a>
</p>
</p>

</body>
</html>


<!-- https://www.umass.edu/it/support/web-hosting/connect-oit-web-hosting-servers-ssh-terminal-macintosh -->
<!-- Connect to website -->

<!-- https://www.digitalocean.com/community/tutorials/how-to-use-sftp-to-securely-transfer-files-with-a-remote-server-es -->
<!-- lls
lpwd
lcd
put index.html /home/tgeffner/public_html
 -->
