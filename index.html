<!DOCTYPE html>
<head>
<!-- The title tag is also shown in browser's tab-->
<title>Tomas Geffner | Research Scientist @ NVIDIA</title>
</head>


<body>
<h1>
Tomas Geffner
</h1>
<p>
<!-- Replace with a personal image if desired -->
<!-- <img src="seminole.png" width="300" > -->
<img src="me.jpg" width="300" >
(After an intense hike)
</p>

<hr>



<!-- I finished my PhD in 2023. I am now a research scientist at NVIDIA, working on generative modeling within the team lead by <a href="http://latentspace.cc/">Arash Vahdat</a>. -->



<h1>
About
</h1>
<p>
<!-- <del>I am a final year PhD student in the <a href="https://www.cics.umass.edu">College of Information and Computer Sciences</a> at UMass Amherst. I am fortunate to be advised by Justin Domke.</del>
<br><br>
I finished my PhD in 2023. I am now a research scientist at NVIDIA, working on generative modeling within the <a href="https://research.nvidia.com/labs/genair/">Fundamental Generative AI Research Group</a>. -->
I am a research scientist at NVIDIA, working on generative modeling within the <a href="https://research.nvidia.com/labs/genair/">Fundamental Generative AI Research Group</a>.
<br><br>
I obtained my PhD in Computer Science from UMass Amherst in 2023. During my PhD I am grateful to have interned at VantAI (New York, summer 2023) with Luca Naef and Michael Bronstein; DeepMind (London, summer 2022) with Andriy Mnih and George Papamakarios; MSR (Cambridge UK, summer 2021) with Cheng Zhang, Emre Kiciman and Miltos Allamanis; and Amazon AWS (New York, summer 2018) with â€ªBing Xiang and Ramesh Nallapati.
<br><br>
I work on probabilistic Machine Learning, focusing on generative models and sampling methods. I am interested in fundamental methodological developments as well as applications in different scientific domains.
<!-- <br><br>
<i>Sampling and approximate inference.</i> I worked on improving sampling and approximate inference techniques, focusing on developing methods that are faster, more robust, flexible, scalable, and overall easier to use.
<br><br>
<i>Generative modeling and causality.</i> I worked on developing methods for conditional generation based on compositional score modeling (applied to simulation-based inference), on causal discovery using normalizing flows and GNNs, and on multi-modal generation and representation learning using VAEs. -->
<br><br>
Before my PhD I also did some work on planning.
<br><br>
This is a <a href="https://scholar.google.com/citations?user=KIIe2K8AAAAJ&hl=en">link</a> to my google scholar, and this is a <a href="https://x.com/tomasgeffner">link</a> to my (somewhat unused) X/Twitter profile.
</p>









<!-- <h1>
About
</h1>
<p>
I am a final year PhD student in the <a href="https://www.cics.umass.edu">College of Information and Computer Sciences</a> at UMass Amherst. I am fortunate to be advised by <a href="https://people.cs.umass.edu/~domke/">Justin Domke</a>.
<br><br>
Most of my work during my PhD has been focused around approximate inference methods. Can we make them faster? More flexible? More robust? Easier to use? 
<br><br>
I'm also interested in generative models and causality. I worked with diffusion models for conditional generation during an internship at DeepMind (summer 2022), with flow-based methods and GNNs for causal discovery during an internship at MSR (summer 2021), and with VAEs for multi modal data during an internship at Amazon AWS (summer 2018).
<br><br>
Before my PhD I also did some work on planning.
<br><br>
This is a <a href="https://scholar.google.com/citations?user=KIIe2K8AAAAJ&hl=es">link</a> to my google scholar.
</p> -->

<hr>

<!-- this is an h1 heading, largest-->
<!-- <h1>
Publications (& Preprints)
</h1>
<p>
<ul style="list-style-type:square;">
<li><a href="https://arxiv.org/abs/2407.01648">Aligning target-aware molecule diffusion models with exact energy optimization.</a> By Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, Tomas Geffner, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon. <br>NeurIPS 2024.<br>We propose a method to finetune diffusion models on rewards</li> <br>
<li><a href="https://arxiv.org/abs/2209.14249">Compositional Score modeling for Simulation-based Inference.</a> By Tomas Geffner, George Papamakarios and Andriy Mnih. <br>ICML 2023.<br>We introduce a novel method to do simulation-based inference (aka likelihood-free inference) using conditional score modeling. Our method learns the score of (diffused versions) of the target posterior distribution, uses annealed Langevin dynamics to get samples, and can naturally aggregate multiple observations at inference time by exploiting different factorizations of the target distribution.</li> <br>
<li><a href="https://arxiv.org/abs/2208.07743">Langevin Diffusion Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2023.<br>We present a new method to combine VI and MCMC based on the discretization of Langevin diffusion processes and their time reversals. We show that our approach generalizes previously existing methods, and outperforms them on a range of inference tasks.</li> <br>
<li><a href="https://arxiv.org/abs/2203.04432">Variational Inference with Locally Enhanced Bounds for Hierarchical Models.</a> By Tomas Geffner and Justin Domke. <br>ICML 2022 (spotlight).<br>We present a new family of variational objectives for hierarchical models, based on the application of importance weighting (and other tightening methods) at the local variables level. Our approach yields tight lower bounds and powerful posterior approximations while being compatible with subsampling, thus addressing one of the main limitations of tightening methods.</li> <br>
<li><a href="https://arxiv.org/abs/2202.02195">Deep End-to-end Causal Inference.</a> By Tomas Geffner*, Javier Antoran*, Adam Foster*, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltadis Allamanis and Cheng Zhang. <br>Workshop on Causality for Real World Impact (NeurIPS 2022).<br>We present a new end-to-end simulation-based method for causal inference. Our approach performs causal discovery by fitting autoregressive flows, and uses the learned model to generate samples from interventional distributions and estimate treatment effects.</li> <br>
<li><a href="https://arxiv.org/abs/2107.04150">MCMC Variational Inference via Uncorrected Hamiltonian Annealing.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2021.<br>We present a new way of combining VI and MCMC using Annealed Importance Sampling with uncorrected HMC transitions. The method is simple to implement, and all of its parameters (even the ones from the HMC transition kernel!) can be tuned with unbiased reparameterization gradients.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>ICML 2021.<br>Want to minimize an alpha divergence using unbiased gradients? It will be hard. In high dimensions your gradient estimator will likely suffer from an exponentially small SNR.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>AABI 2020 (contributed talk).<br>Want to minimize an alpha divergence using biased gradients? It will be hard. In high dimensions you may end up minimizing the typical exclusive KL divergence.</li> <br>
<li><a href="https://arxiv.org/abs/2007.14634">Approximation Based Variance Reduction for Reparameterization Gradients.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2020.<br>We propose a new control variate to reduce the variance of gradient estimators used by VI. This leads to faster convergence. Our method works better than previously proposed methods based on Taylor expansions. We propose an explanation for this.</li> <br>
<li><a href="https://arxiv.org/abs/1911.01894">A Rule for Gradient Estimator Selection, with an Application to Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2020.<br>Some gradient estimates are cheap but have a high variance. Some are expensive but have lower variance. Others are somewhere in between. Which one should you use? We study this using known SGD convergence rates and propose a simple gradient estimator selection algorithm that aims to minimize optimization wall-clock time.</li> <br>
<li><a href="https://arxiv.org/abs/1810.12482">Using Large Ensembles of Control Variates for Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2018.<br>Using a control variate may lead to a reduction in the variance of gradient estimators, improving optimization convergence. For VI, many control variates are available. We propose a method to use them jointly and show that this may work better than any of the control variates individually.</li> <br>
<li><a href="https://arxiv.org/abs/1806.09455">Compact Policies for Fully-Observable Non-Deterministic Planning as SAT.</a> By Tomas Geffner and Hector Geffner. <br>ICAPS 2018.<br>We introduce a new SAT-based algorithm for FOND planning. The proposed method works well on several probabilistic interesting domains.</li> <br>
<li><a href="http://giga15.ru.is/giga15-paper2.pdf">Width-based Planning for General Video-Game Playing.</a> By Tomas Geffner and Hector Geffner. <br>AAIDE 2015.<br>Developed agent for GVG-AI planning competition (real time autonomous game playing) based on a combination of planning and learning algorithms. The method got the 1st place in CEEC leg of competition (2015).</li> <br>
</ul>
</p> -->

<!-- <h1>
Publications (& Preprints)
</h1>
<p>
<ul style="list-style-type:square;">
<li><a href="https://arxiv.org/pdf/2410.21357">Energy-Based Diffusion Language Models for Text Generation.</a> By Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat. <br>ICLR 2025.<br>Method to improve discrete diffusions through the use of enery-based models.</li> <br>
<li><a href="https://arxiv.org/abs/2410.14895">Truncated consistency models.</a> By Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, Weili Nie. <br>ICLR 2025.<br>New method to train consistency models.</li> <br>
<li><a href="https://www.biorxiv.org/content/10.1101/2024.07.17.603980v4">PINDER: The protein interaction dataset and evaluation resource.</a> By Daniel Kovtun, Mehmet Akdel, Alexander Goncearenco, Guoqing Zhou, Graham Holt, David Baugher, Dejun Lin, Yusuf Adeshina, Thomas Castiglione, Xiaoyun Wang, Celine Marquet, Matt McPartlon, Tomas Geffner, Emanuele Rossi, Gabriele Corso, Hannes Stark, Zachary Carpenter, Emine Kucukbenli, Michael Bronstein, Luca Naef. <br>Biorxiv (2024).<br>New dataset for protein-protein interactions.</li> <br>
<li><a href="https://arxiv.org/abs/2407.01648">Aligning target-aware molecule diffusion models with exact energy optimization.</a> By Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, Tomas Geffner, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon. <br>NeurIPS 2024.<br>Finetuning diffusion models using user-specified rewards, applied to molecule generation.</li> <br>
<li><a href="https://arxiv.org/abs/2209.14249">Compositional Score modeling for Simulation-based Inference.</a> By Tomas Geffner, George Papamakarios and Andriy Mnih. <br>ICML 2023.<br>Novel approach to compose diffusion models, applied to simulation-based inference.</li> <br>
<li><a href="https://arxiv.org/abs/2208.07743">Langevin Diffusion Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2023.<br>New method to combine variational inference and MCMC based on the discretization of Langevin diffusion processes and their time reversals.</li> <br>
<li><a href="https://arxiv.org/abs/2203.04432">Variational Inference with Locally Enhanced Bounds for Hierarchical Models.</a> By Tomas Geffner and Justin Domke. <br>ICML 2022 (spotlight).<br>New family of variational objectives for hierarchical models, based on the application of importance weighting (and other tightening methods) at the local variables level.</li> <br>
<li><a href="https://arxiv.org/abs/2202.02195">Deep End-to-end Causal Inference.</a> By Tomas Geffner*, Javier Antoran*, Adam Foster*, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltadis Allamanis and Cheng Zhang. <br>TMLR.<br>We present a new end-to-end simulation-based method for causal inference, based on Bayesian causal discovery, autoregressive flows, and Monte Carlo estimation for treatment effect estimation.</li> <br>
<li><a href="https://arxiv.org/abs/2107.04150">MCMC Variational Inference via Uncorrected Hamiltonian Annealing.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2021.<br>Novel method to compbine variatinal inference and MCMC using Annealed Importance Sampling with uncorrected HMC transitions.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>ICML 2021.<br>Theoretical analysis of methods based on unbiased gradient estimators to minimize alpha divergences. Main conclusion: In (moderately) high dimensions your gradient estimator will likely suffer from an exponentially small SNR.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>AABI 2020 (contributed talk).<br>Thorough and illustrative empirical evaluation of multiple methods based on biased gradient estimators to minimize alpha divergences. Main conclusion: In (moderately) high dimensions you may end up minimizing the typical exclusive KL divergence.</li> <br>
<li><a href="https://arxiv.org/abs/2007.14634">Approximation Based Variance Reduction for Reparameterization Gradients.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2020.<br>Improved gradient estimation in variaional inference through a novel control variate.</li> <br>
<li><a href="https://arxiv.org/abs/1911.01894">A Rule for Gradient Estimator Selection, with an Application to Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>AISTATS 2020.<br>We propose a method to balance gradient cost and variance to maximize optimization speed.</li> <br>
<li><a href="https://arxiv.org/abs/1810.12482">Using Large Ensembles of Control Variates for Variational Inference.</a> By Tomas Geffner and Justin Domke. <br>NeurIPS 2018.<br>We propose improved gradient estimation for variational inference through the use of multiple control variates in concert.</li> <br>
<li><a href="https://arxiv.org/abs/1806.09455">Compact Policies for Fully-Observable Non-Deterministic Planning as SAT.</a> By Tomas Geffner and Hector Geffner. <br>ICAPS 2018.<br>New SAT-based algorithm for FOND planning.</li> <br>
<li><a href="http://giga15.ru.is/giga15-paper2.pdf">Width-based Planning for General Video-Game Playing.</a> By Tomas Geffner and Hector Geffner. <br>AAIDE 2015.<br>Agent for GVG-AI planning competition (real time autonomous game playing) based on a combination of planning and learning algorithms..</li> <br>
</ul>
</p> -->

<!-- <h1>
Publications (& Preprints)
</h1>
<p>
<ul style="list-style-type:square;">
<li><a href="https://arxiv.org/pdf/2410.21357">Energy-Based Diffusion Language Models for Text Generation.</a> By Minkai Xu, <u>Tomas Geffner</u>, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat. <br>ICLR 2025.</li> <br>
<li><a href="https://arxiv.org/abs/2410.14895">Truncated Consistency Models.</a> By Sangyun Lee, Yilun Xu, <u>Tomas Geffner</u>, Giulia Fanti, Karsten Kreis, Arash Vahdat, Weili Nie. <br>ICLR 2025.</li> <br>
<li><a href="https://www.biorxiv.org/content/10.1101/2024.07.17.603980v4">PINDER: The protein interaction dataset and evaluation resource.</a> By Daniel Kovtun, Mehmet Akdel, Alexander Goncearenco, Guoqing Zhou, Graham Holt, David Baugher, Dejun Lin, Yusuf Adeshina, Thomas Castiglione, Xiaoyun Wang, Celine Marquet, Matt McPartlon, <u>Tomas Geffner</u>, Emanuele Rossi, Gabriele Corso, Hannes Stark, Zachary Carpenter, Emine Kucukbenli, Michael Bronstein, Luca Naef. <br>Biorxiv (2024).</li> <br>
<li><a href="https://arxiv.org/abs/2407.01648">Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization.</a> By Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, <u>Tomas Geffner</u>, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon. <br>NeurIPS 2024.</li> <br>
<li><a href="https://arxiv.org/abs/2209.14249">Compositional Score modeling for Simulation-based Inference.</a> By <u>Tomas Geffner</u>, George Papamakarios and Andriy Mnih. <br>ICML 2023.</li> <br>
<li><a href="https://arxiv.org/abs/2208.07743">Langevin Diffusion Variational Inference.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>AISTATS 2023.</li> <br>
<li><a href="https://arxiv.org/abs/2203.04432">Variational Inference with Locally Enhanced Bounds for Hierarchical Models.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>ICML 2022 (spotlight).</li> <br>
<li><a href="https://arxiv.org/abs/2202.02195">Deep End-to-end Causal Inference.</a> By <u>Tomas Geffner</u>*, Javier Antoran*, Adam Foster*, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltadis Allamanis and Cheng Zhang. <br>TMLR.</li> <br>
<li><a href="https://arxiv.org/abs/2107.04150">MCMC Variational Inference via Uncorrected Hamiltonian Annealing.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2021.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>ICML 2021.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>AABI 2020 (contributed talk).</li> <br>
<li><a href="https://arxiv.org/abs/2007.14634">Approximation Based Variance Reduction for Reparameterization Gradients.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2020.</li> <br>
<li><a href="https://arxiv.org/abs/1911.01894">A Rule for Gradient Estimator Selection, with an Application to Variational Inference.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>AISTATS 2020.</li> <br>
<li><a href="https://arxiv.org/abs/1810.12482">Using Large Ensembles of Control Variates for Variational Inference.</a> By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2018.</li> <br>
<li><a href="https://arxiv.org/abs/1806.09455">Compact Policies for Fully-Observable Non-Deterministic Planning as SAT.</a> By <u>Tomas Geffner</u> and Hector Geffner. <br>ICAPS 2018.</li> <br>
<li><a href="http://giga15.ru.is/giga15-paper2.pdf">Width-based Planning for General Video-Game Playing.</a> By <u>Tomas Geffner</u> and Hector Geffner. <br>AAIDE 2015.</li> <br>
</ul>
</p> -->

<h1>
Publications (& Preprints)
</h1>
<p>
<ul style="list-style-type:square;">
<li><a href="https://arxiv.org/abs/2507.09466">La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching.</a><br>By <u>Tomas Geffner</u>*,  Kieran Didi*, Zhonglin Cao, Danny Reidenbach, Zuobai Zhang, Christian Dallago, Emine Kucukbenli, Karsten Kreis, Arash Vahdat. <br>Arxiv (2025).</li> <br>
<li><a href="https://arxiv.org/abs/2503.20719">Learning Straight Flows by Learning Curved Interpolants.</a><br>By Shiv Shankar, <u>Tomas Geffner</u>. <br>Arxiv (2025).</li> <br>
<li><a href="https://openreview.net/pdf?id=6uPcJtMgWN">Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow.</a><br>By Zhonglin Cao, Mario Geiger, Allan Dos Santos Costa, Danny Reidenbach, Karsten Kreis, <u>Tomas Geffner</u>, Franco Pellegrini, Guoqing Zhou, Emine Kucukbenli. <br>ICML (2025).</li> <br>
<li><a href="https://arxiv.org/abs/2410.19814">Adaptive Flow Matching for Resolving Small-Scale Physics.</a><br>By Stathi Fotiadis, Noah Brenowits, <u>Tomas Geffner</u>, Yair Cohen, Michael Protchard, Arash Vahdat, Morteza Mardani. <br>ICML (2025).</li> <br>
<li><a href="https://openreview.net/pdf?id=TVQLu34bdw">Proteina: Scaling Flow-based Protein Structure Generative Models.</a><br>By <u>Tomas Geffner</u>*, Kieran Didi*, Zuobai Zhang*, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, Karsten Kreis*. <br>ICLR 2025 (<b>oral, top 1.8%</b>).</li> <br>
<li><a href="https://openreview.net/pdf?id=0ctvBgKFgc">ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids.</a><br>By Hannes Stark*, Bowen Jing*, <u>Tomas Geffner</u>, Jason Yim, Tommi Jaakola, Arash Vahdat, Karsten Kreis. <br>ICLR 2025 (<b>oral, top 1.8%</b>).</li> <br>
<li><a href="https://arxiv.org/pdf/2410.21357">Energy-Based Diffusion Language Models for Text Generation.</a><br>By Minkai Xu, <u>Tomas Geffner</u>, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat. <br>ICLR 2025.</li> <br>
<li><a href="https://arxiv.org/abs/2410.14895">Truncated Consistency Models.</a><br>By Sangyun Lee, Yilun Xu, <u>Tomas Geffner</u>, Giulia Fanti, Karsten Kreis, Arash Vahdat, Weili Nie. <br>ICLR 2025.</li> <br>
<li><a href="https://www.biorxiv.org/content/10.1101/2024.07.17.603980v4">PINDER: The protein interaction dataset and evaluation resource.</a><br>By Daniel Kovtun, Mehmet Akdel, Alexander Goncearenco, Guoqing Zhou, Graham Holt, David Baugher, Dejun Lin, Yusuf Adeshina, Thomas Castiglione, Xiaoyun Wang, Celine Marquet, Matt McPartlon, <u>Tomas Geffner</u>, Emanuele Rossi, Gabriele Corso, Hannes Stark, Zachary Carpenter, Emine Kucukbenli, Michael Bronstein, Luca Naef. <br>Biorxiv (2024).</li> <br>
<li><a href="https://arxiv.org/abs/2407.01648">Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization.</a><br>By Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, <u>Tomas Geffner</u>, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon. <br>NeurIPS 2024.</li> <br>
<li><a href="https://www.mlsb.io/papers_2023/LatentDock_Protein-Protein_Docking_with_Latent_Diffusion.pdf">LatentDock: Protein-Protein Docking with Latent Diffusion.</a><br>By Matt McPartlon, Celine Marquet, <u>Tomas Geffner</u>, Daniel Kovtun, Alexander Goncearenco, Zachary Carpenter, Luca Naef, Michael Bronstein, Jinbo Xu. <br>Machine Learning for Structural Biology Workshop (NeurIPS 2023).</li> <br>
<li><a href="https://openreview.net/pdf?id=PQa3giMLZp">Bending and Binding: Predicting Protein Flexibility upon Ligand Interaction using Diffusion Models.</a><br>By Xuejin Zhang, <u>Tomas Geffner</u>, Matt McPartlon, Mehmet Akdel, Dylan Abramson, Graham Holt, Alexander Goncearenco, Luca Naef, Michael Bronstein. <br>Generative AI and Biology Workshop (NeurIPS 2023).</li> <br>
<li><a href="https://arxiv.org/abs/2209.14249">Compositional Score modeling for Simulation-based Inference.</a><br>By <u>Tomas Geffner</u>, George Papamakarios and Andriy Mnih. <br>ICML 2023.</li> <br>
<li><a href="https://arxiv.org/abs/2208.07743">Langevin Diffusion Variational Inference.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>AISTATS 2023.</li> <br>
<li><a href="https://arxiv.org/abs/2203.04432">Variational Inference with Locally Enhanced Bounds for Hierarchical Models.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>ICML 2022 (spotlight).</li> <br>
<li><a href="https://arxiv.org/abs/2202.02195">Deep End-to-end Causal Inference.</a><br>By <u>Tomas Geffner</u>*, Javier Antoran*, Adam Foster*, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltadis Allamanis and Cheng Zhang. <br>TMLR.</li> <br>
<li><a href="https://arxiv.org/abs/2107.04150">MCMC Variational Inference via Uncorrected Hamiltonian Annealing.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2021.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>ICML 2021.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>AABI 2020 (contributed talk).</li> <br>
<li><a href="https://arxiv.org/abs/2007.14634">Approximation Based Variance Reduction for Reparameterization Gradients.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2020.</li> <br>
<li><a href="https://arxiv.org/abs/1911.01894">A Rule for Gradient Estimator Selection, with an Application to Variational Inference.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>AISTATS 2020.</li> <br>
<li><a href="https://arxiv.org/abs/1810.12482">Using Large Ensembles of Control Variates for Variational Inference.</a><br>By <u>Tomas Geffner</u> and Justin Domke. <br>NeurIPS 2018.</li> <br>
<li><a href="https://arxiv.org/abs/1806.09455">Compact Policies for Fully-Observable Non-Deterministic Planning as SAT.</a><br>By <u>Tomas Geffner</u> and Hector Geffner. <br>ICAPS 2018.</li> <br>
<li><a href="http://giga15.ru.is/giga15-paper2.pdf">Width-based Planning for General Video-Game Playing.</a><br>By <u>Tomas Geffner</u> and Hector Geffner. <br>AAIDE 2015.</li> <br>
</ul>
</p>

<!-- <hr> -->

<!-- 
<h1>
Other interesting projects	
</h1>
<p>
<ul style="list-style-type:square;">
<li>Bayesian Modeling to Infer User Preferences. We use a hierarchical probabilistic model to automatically infer users' preferences. The proposed model allows parameter sharing between users and the incorporation of domain knowledge.</li> <br>
<li><a href="https://arxiv.org/abs/2105.06587">Empirical Evaluation of Biased Methods for Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>AABI 2020 (contributed talk).<br>Want to minimize an alpha divergence using biased gradients? It will be hard. In high dimensions you may end up minimizing the typical exclusive KL divergence.</li> <br>
<li><a href="https://arxiv.org/abs/2010.09541">On the Difficulty of Unbiased Alpha Divergence Minimization.</a> By Tomas Geffner and Justin Domke. <br>ICML 2021.<br>Want to minimize an alpha divergence using unbiased gradients? It will be hard. In high dimensions your gradient estimator will likely suffer from an exponentially small SNR.</li> <br>
</p>
 -->

<hr>

<h1>
Service
</h1>
Organizing Committee:
<ul style="list-style-type:none;">
<li>Publication chair (AISTATS 2023)</li>
</ul>
<br>
Conference reviewing:
<ul style="list-style-type:none;">
<li>NeurIPS (2019, 2020, 2021, 2022, 2025)</li>
<li>ICML (2020, 2025)</li>
<li>ICLR (2021, 2022, 2025)</li>
<li>AISTATS (2021)</li>
</ul>
<br>
Journal reviewing:
<ul style="list-style-type:none;">
<li>JMLR</li>
<li>TMLR</li>
</ul>

<!-- <hr>

<h1>
Honors and Awards
</h1>
<ul style="list-style-type:circle;">
<li>Winner of the 97th annual FSU Math Competition</li>
<li>Received a teaching award for best TA for the Spring 2018 semester.</li>
<li>Recipient of the FSU Fellowship award.</li>
</ul> -->

<hr>

<!-- <h1>
Teaching experience
</h1>
I was a TA for several courses:
<ul style="list-style-type:none;">
<li>Probabilistic graphical models. Office hours and prepared course notes.</li>
<li>Machine learning. Prepared homeworks and exams.</li>
<li>Introduction to data structures. Office hours, discussion sessions, prepared and graded homeworks.</li>
<li>Introduction to Natural language processing. Office hours and grading.</li>
<li>Human Computer Interaction. Mostly grading.</li>
</ul>

<hr> -->

<h1>
Teaching experience
</h1>
I was a teaching assistant for several courses:
<ul style="list-style-type:none;">
<li>Probabilistic graphical models.</li>
<li>Machine learning.</li>
<li>Introduction to data structures.</li>
<li>Introduction to Natural language processing.</li>
<li>Human Computer Interaction.</li>
</ul>

<hr>

<h1>
Other
</h1>
<p>
I try to play tennis.
</p>
</p>

<hr>

<h1>
Contact
</h1>
<table>
<!-- <tr><td><bold>Email:</bold> t[lastname]@cs.umass.edu</td> </tr> -->
<tr><td><bold>Email:</bold> t[lastname]@nvidia.com</td> </tr>
<tr><td><bold>Email:</bold> [firstname][lastname]@gmail.com</td> </tr>
</tr>
</table>

<hr>

<h1>
Disclaimer
</h1>
<p>
The template for this website was obtained from <a href="https://www.math.fsu.edu/Computer/personal-webpage-templates/">here.</a>
</p>
</p>

</body>
</html>


<!-- https://www.umass.edu/it/support/web-hosting/connect-oit-web-hosting-servers-ssh-terminal-macintosh -->
<!-- Connect to website -->

<!-- https://www.digitalocean.com/community/tutorials/how-to-use-sftp-to-securely-transfer-files-with-a-remote-server-es -->
<!-- lls
lpwd
lcd
put index.html /home/tgeffner/public_html
 -->
